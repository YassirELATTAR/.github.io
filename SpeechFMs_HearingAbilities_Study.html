<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Hearing Abilities of Foundation Models</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link
    href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i"
    rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/remixicon/remixicon.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">


</head>

<body>

  <main id="main">

    <!-- ======= Portfolio Details ======= -->
    <div id="portfolio-details" class="portfolio-details">
      <div class="container">

        <div class="row">

          <div class="col-lg-8">
            <h2 class="portfolio-title">Hearing Abilities of Foundation Models</h2>

            <div class="portfolio-details-slider swiper">
              <div class="swiper-wrapper align-items-center">
                <div class="swiper-slide">
                  <img src="assets/img/portfolio/hearingAbilities-of-FoundationModels-GeneratedBy-CoPilot.png" alt="Foundation Models Hearing Abilities - Generated by copilot">
                </div>
                <!--
                <div class="swiper-slide">
                  <img src="assets/img/portfolio/salmonn.png" alt="">
                </div>
                <div class="swiper-slide">
                  <img src="assets/img/portfolio/DynamicSUPERB.png" alt="">
                </div>

                <div class="swiper-slide">
                  <img src="assets/img/portfolio/LargeScaleEvaluation.png" alt="">
                </div>


                <div class="swiper-slide">
                  <img src="assets/img/portfolio/speaker_vs_speech-Models.png" alt="">
                </div>
              -->

              </div>
              <div class="swiper-pagination"></div>
            </div>

          </div>

          <div class="col-lg-4 portfolio-info">
            <h3>Project information</h3>
            <ul>
              <li><strong>Category</strong>: Course Project</li>
              <li><strong>Course</strong>: Current Topics in Speech Technology</li>
              <li><strong>Project date</strong>: January, 2025</li>
              <li><strong>Main Papers</strong>: <a href="#references">References</a></li>
            </ul>
            <h2>Overview:</h2>
            <p>
              Hearing is an essential skill for artificial intelligence agents functioning in the physical world. It
              encompasses the ability to perceive and understand a wide range of auditory information. This not only
              includes spoken language but also extends to other audio events and music. In this project, we aim to
              delve into the auditory capabilities of cutting-edge spoken language models, exploring the scope of what
              they can hear. By conducting systematic assessments on benchmark hearing tasks, we will evaluate the
              performance of these models. Additionally, this exploration will help us investigate the hearing abilities
              of these models more deeply, understand their limitations, and identify any potential gaps in current
              research. This thorough understanding could lead to significant enhancements in how AI systems interact
              with their environment through sound.
            </p>
          </div>

        </div>
        <div class="row">

          <!-- New detailed sections -->
          <div class="section-details">
            <div class="container">

              <!-- SALMONN: Towards Generic Hearing Abilities of LLMs -->
              <div class="row">
                <div class="col-lg-12">
                  <h2>SALMONN: Towards Generic Hearing Abilities of LLMs <a href="#ref-salmonn">(Tang et al., 2024)</a>
                  </h2>
                  <p>
                    <strong>Abstract:</strong> SALMONN is an innovative model that integrates large language models
                    (LLMs) with dual auditory encoders to achieve advanced auditory comprehension. It processes diverse
                    inputs including speech, audio events, and music, excelling in both trained tasks (e.g., speech
                    recognition, audio captioning) and untrained tasks such as audio storytelling and co-reasoning.
                    <i>(<a href="https://github.com/bytedance/SALMONN">model's github page</a>)</i>
                  </p>
                  <p>
                    This section provides a detailed breakdown of the paper, highlighting key architectural components,
                    methodology, and results.
                  </p>
                </div>
              </div>

              <!-- Architecture -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Model Architecture</h3>
                  <p>
                    The SALMONN architecture employs a dual encoder system integrating OpenAI's Whisper for speech
                    processing and BEATs for non-speech audio. Outputs are synchronized via a window-level Q-Former
                    module, ensuring temporal alignment. The auditory tokens are combined with textual inputs and
                    processed by a pre-trained Vicuna LLM, which is adapted for multimodal inputs using LoRA.
                  </p>
                  <p>
                    Key innovations include:
                  </p>
                  <ul>
                    <li>Dual encoder system for complementary speech and non-speech processing.</li>
                    <li>Window-level Q-Former for efficient handling of variable-length audio inputs.</li>
                    <li>LoRA-based adaptation for integrating auditory embeddings with LLMs.</li>
                  </ul>
                  <div style="text-align:center; margin: 20px 0;">
                    <img src="assets/img/salmonn-architecture.png" alt="SALMONN Architecture" class="figures-image"
                      style="display: block; margin: 0 auto;" />
                    <p class="image-caption">Figure 1: SALMONN model architecture, showcasing its dual encoders and
                      Q-Former integration with Vicuna LLM.</p>
                  </div>
                </div>
              </div>

              <!-- Methodology -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Methodology</h3>
                  <p>
                    SALMONN's training methodology is divided into three stages:
                  </p>
                  <ol>
                    <li><strong>Pre-training:</strong> Focused on speech recognition and audio captioning to establish
                      high-quality audio-text alignment.</li>
                    <li><strong>Instruction Tuning:</strong> Multi-task training with supervised data, emphasizing
                      cross-modal capabilities like translation and emotion recognition.</li>
                    <li><strong>Activation Tuning:</strong> Fine-tuning to activate emergent abilities for untrained
                      tasks, addressing task overfitting issues.</li>
                  </ol>
                  <p>
                    This phased approach ensures the model achieves robust performance on both trained and novel
                    auditory tasks.
                  </p>

                  <div style="text-align:center; margin: 20px 0;">
                    <img src="assets/img/salmonn-setup.png" alt="SALMONN Setup" class="figures-image"
                      style="display: block; margin: 0 auto;" />
                    <p class="image-caption">Figure 2: Tasks (the three levels), Test Data, Evaluation Metrics, and The
                      Reference Value.</p>
                  </div>
                </div>
              </div>

              <!-- Results -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Results</h3>
                  <p>
                    SALMONN demonstrated competitive performance across a diverse range of benchmarks:
                  </p>
                  <ul>
                    <li>Trained Tasks: Achieved state-of-the-art results in automatic speech recognition, audio
                      captioning, and speech translation.</li>
                    <li>Untrained Tasks: Successfully handled storytelling, audio co-reasoning, and multilingual
                      translation.</li>
                    <li>Emergent Abilities: Exhibited innovative capabilities such as audio-based storytelling and
                      complex question answering.</li>
                  </ul>
                  <div style="text-align:center; margin: 20px 0;">
                    <img src="assets/img/salmonn-results.png" alt="SALMONN Results" class="figures-image"
                      style="display: block; margin: 0 auto;" />
                    <p class="image-caption">Figure 3: Results summary showcasing SALMONN's performance across trained
                      and untrained tasks.</p>
                  </div>
                </div>
              </div>

              <!-- Examples -->
              <div class="row">
                <div class="col-lg-12">
                  <h3><i>Examples of SALMONN on Level 3 Tasks</i></h3>
                  <div style="text-align:center; margin: 20px 0;">
                    <img src="assets/img/salmonn-example-story.png" alt="SALMONN Results" class="figures-image"
                      style="display: block; margin: 0 auto;" />
                    <p class="image-caption">Figure 4: Audio Story Telling.</p>
                  </div>
                  <div style="text-align:center; margin: 20px 0;">
                    <img src="assets/img/salmonn-example-sac.png" alt="SALMONN Results" class="figures-image"
                      style="display: block; margin: 0 auto;" />
                    <p class="image-caption">Figure 5: Speech Audio Coreasoning.</p>
                  </div>
                </div>
              </div>

              <!-- Conclusion -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Conclusion</h3>
                  <p>
                    SALMONN sets a new standard in multimodal AI research, bridging auditory and linguistic processing.
                    Its architecture and training methodology provide a foundation for developing generic hearing
                    capabilities in AI.
                  </p>
                </div>
              </div>
              <hr style="border: 1px solid #ccc; margin: 40px 0;">

              <!-- What Do Self-Supervised Speech and Speaker Models Learn? -->
              <div class="row">
                <div class="col-lg-12">
                  <h2>What Do Self-Supervised Speech and Speaker Models Learn? <a href="#ref-ashira">(Ashihara et al.,
                      2024)</a></h2>
                  <p>
                    <strong>Abstract:</strong> This paper explores the capabilities of self-supervised learning (SSL)
                    models in representing speech and speaker characteristics. The study focuses on understanding how
                    these models capture linguistic and speaker-specific features, using cross-model and layer-wise
                    analysis. The results shed light on the fundamental differences between speech and speaker SSL
                    models, as well as fully supervised models.
                  </p>
                  <p>
                    Below is a comprehensive breakdown of the findings, architectural details, and evaluation
                    methodologies presented in the paper.
                  </p>
                </div>
              </div>

              <!-- Architecture -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Model Architectures</h3>
                  <p>
                    The study evaluates three key types of models:
                  </p>
                  <ul>
                    <li><strong>Speech SSL Models:</strong> HuBERT and WavLM, designed for general-purpose speech
                      representation using masked prediction training.</li>
                    <li><strong>Speaker SSL Models:</strong> Based on the ECAPA-TDNN architecture, optimized for
                      speaker-specific tasks using non-contrastive learning frameworks such as DINO.</li>
                    <li><strong>Supervised Speaker Models:</strong> Conventional models trained with speaker-specific
                      supervised objectives like angular additive margin softmax (AAM-Softmax).</li>
                  </ul>
                  <p>
                    The architectural differences focus on how the models disentangle speaker and linguistic features at
                    different layers. For instance, ECAPA-TDNN uses frame-level blocks and an attentive statistics
                    pooling layer for utterance-level embeddings, whereas speech SSL models employ deeper
                    transformer-based architectures.
                  </p>
                  <div style="text-align:center; margin: 20px 0;">
                    <img src="assets/img/ENCAPA-TDNN-speakerModel.png" alt="ECAPA-TDNN Architecture"
                      class="figures-image" style="display: block; margin: 0 auto;" />
                    <p class="image-caption">Figure 1: Architecture of ECAPA-TDNN used for speaker models.</p>
                  </div>
                  <div style="text-align:center; margin: 20px 0;">
                    <img src="assets/img/DINO-speakerModel.png" alt="Speech SSL Model Architecture"
                      class="figures-image" style="display: block; margin: 0 auto;" />
                    <p class="image-caption">Figure 2: Typical architecture of Speech SSL models.
                    </p>
                  </div>
                </div>
              </div>

              <!-- Methodology -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Methodology</h3>
                  <p>
                    Two main analysis methods were employed:
                  </p>
                  <ol>
                    <li>
                      <strong>SUPERB Evaluation Probing Tasks:</strong> A set of tasks from the SUPERB benchmark was
                      used to probe how different models represent speech and speaker information. Tasks included:
                      <ul>
                        <li>Phoneme Recognition (PR) for content evaluation.</li>
                        <li>Speaker Identification (SID) and Automatic Speaker Verification (ASV) for speaker-specific
                          evaluation.</li>
                        <li>Intent Classification (IC) and Emotion Recognition (ER) for semantic and paralinguistic
                          analysis.</li>
                      </ul>
                    </li>
                    <li>
                      <strong>Layer-Wise Similarity Analysis:</strong> Using the Linear Centered Kernel Alignment
                      (LinCKA) method, the study measured similarities between layers within and across models to
                      understand how information is distributed.
                    </li>
                  </ol>
                  <p>
                    Additionally, the models were trained and evaluated using datasets such as VoxCeleb2, LibriSpeech,
                    and Libri-Light.
                  </p>
                </div>
              </div>

              <!-- Results -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Results</h3>
                  <p>
                    The study revealed key insights into how different models process information:
                  </p>
                  <ul>
                    <li>Speech SSL models like WavLM LARGE excel at linguistic tasks but are less optimized for
                      speaker-specific tasks.</li>
                    <li>Speaker SSL models, while adept at capturing speaker-specific features, struggle with linguistic
                      information.</li>
                    <li>Fully supervised speaker models achieved the best performance on speaker tasks but lacked
                      generalization to non-speaker tasks.</li>
                  </ul>
                  <p>
                    Visualization of layer contributions and similarity analysis highlighted the hierarchical nature of
                    feature extraction in SSL models. For instance:
                  </p>
                  <div style="text-align:center; margin: 20px 0;">
                    <img src="assets/img/speaker-vs-speech-layer-contributions.png" alt="Layer Contribution Weights"
                      class="figures-image" style="display: block; margin: 0 auto;" />
                    <p class="image-caption">Figure 3: Layer contribution weights for WavLM LARGE, DINO-based speaker
                      SSL, and supervised speaker models.</p>
                  </div>
                  <div style="text-align:center; margin: 20px 0;">
                    <img src="assets/img/speaker-vs-speech-layer-wise-similarity.png" alt="Layer Similarity Analysis"
                      class="figures-image" style="display: block; margin: 0 auto;" />
                    <p class="image-caption">Figure 4: Layer-wise similarity heatmaps comparing WavLM and speaker
                      models.</p>
                  </div>
                </div>
              </div>

              <!-- Conclusion -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Conclusion</h3>
                  <p>
                    The study provides a comprehensive analysis of how SSL models represent speech and speaker
                    information. It highlights the trade-offs between general-purpose and specialized models and
                    suggests directions for developing more efficient and effective training methods.
                  </p>
                </div>
              </div>

              <hr style="border: 1px solid #ccc; margin: 40px 0;">

              <!-- A Large Scale Evaluation of Speech Foundation Models -->
              <div class="row">
                <div class="col-lg-12">
                  <h2>A Large Scale Evaluation of Speech Foundation Models <a href="#ref-yang">(Yang et al., 2024)</a>
                  </h2>
                  <p>
                    <strong>Abstract:</strong> This paper presents a comprehensive evaluation of Speech Foundation
                    Models using the
                    Speech Processing Universal PERformance Benchmark (SUPERB). The study investigates the
                    generalizability of
                    self-supervised learning (SSL) models across 15 diverse speech tasks, ranging from phoneme
                    recognition to
                    voice conversion. The research introduces a unified benchmarking framework and demonstrates that SSL
                    models
                    achieve state-of-the-art (SOTA) performance with minimal downstream customization.
                  </p>
                  <p>
                    This section explores the architecture, methodology, evaluation, and results discussed in the paper.
                  </p>
                </div>
              </div>

              <!-- Architecture -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Architecture of Speech Foundation Models</h3>
                  <p>
                    The paper evaluates various SSL models, including wav2vec, HuBERT, and WavLM, each leveraging a
                    multi-layer
                    architecture. The hidden representations from all layers are weighted using a learnable weighted-sum
                    mechanism
                    to optimize downstream task performance.
                  </p>
                  <ul>
                    <li><strong>Speech Foundation Model:</strong> Encodes waveforms into multi-layer hidden
                      representations.</li>
                    <li><strong>Prediction Heads:</strong> Lightweight task-specific heads adapt representations for
                      individual tasks.</li>
                    <li><strong>Task Generalization:</strong> Models are frozen, requiring only layer-weight
                      optimization for each task.</li>
                  </ul>
                  <div style="text-align:center; margin: 20px 0;">
                    <img src="assets/img/unified-architecture-of-speech-FMs.png" alt="Architecture Diagram"
                      class="figures-image" style="display: block; margin: 0 auto;" />
                    <p class="image-caption">Figure 1: Unified architecture for Speech Foundation Models with
                      task-specific prediction heads.</p>
                  </div>
                </div>
              </div>

              <!-- Methodology -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Methodology</h3>
                  <p>
                    The SUPERB benchmark standardizes evaluation across 15 tasks, including:
                  </p>
                  <ul>
                    <li>Content Tasks: Phoneme Recognition (PR), Automatic Speech Recognition (ASR), and Keyword
                      Spotting (KS).</li>
                    <li>Speaker Tasks: Speaker Identification (SID), Verification (SV), and Diarization (SD).</li>
                    <li>Generative Tasks: Voice Conversion (VC) and Speech Separation (SS).</li>
                    <li>Semantic Tasks: Intent Classification (IC) and Speech Translation (ST).</li>
                  </ul>
                  <p>
                    The unified framework ensures consistent evaluation by freezing the foundation model and learning
                    task-specific
                    layer weights. Evaluation datasets include LibriSpeech, VoxCeleb, and IEMOCAP, among others.
                  </p>
                </div>
              </div>

              <!-- Results -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Results</h3>
                  <p>
                    The evaluation revealed significant findings:
                  </p>
                  <ul>
                    <li>SSL models, particularly WavLM and HuBERT, outperform traditional approaches on most tasks.</li>
                    <li>Learnable weighted-sum consistently improves performance compared to using the last layer alone.
                    </li>
                    <li>While strong on discriminative tasks, SSL models show a performance gap in generative tasks like
                      SE and SS.</li>
                  </ul>
                  <p>
                    Detailed comparisons highlight WavLM Large achieving near or better results than SOTA non-SSL
                    methods on
                    tasks such as SID, IC, and ASR.
                  </p>
                  <div style="text-align:center; margin: 20px 0;">
                    <img src="assets/img/a-large-scale-evaluation-heatmap.png" alt="Performance Heatmap"
                      class="figures-image" style="display: block; margin: 0 auto;" />
                    <p class="image-caption">Figure 2: Heatmap comparing performance of 33 models across all SUPERB
                      tasks.</p>
                  </div>
                  <div style="text-align:center; margin: 20px 0;">
                    <img src="assets/img/a-large-scale-evaluation-layer-wise-performance.png"
                      alt="Weighted-Sum vs Last Layer Performance" class="figures-image"
                      style="display: block; margin: 0 auto;" />
                    <p class="image-caption">Figure 3: Comparison of each layer's performance to layer-weights after the
                      weighted-sum benchmarking. The blue lines are for SID; the red lines are for PR. The solid lines
                      are for HuBERT Large; the dashed lines are for wav2vec 2.0 Large..</p>
                  </div>
                </div>
              </div>

              <!-- Conclusion -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Conclusion</h3>
                  <p>
                    The study confirms the potential of SSL techniques in developing robust Speech Foundation Models.
                    SUPERB
                    provides a standardized benchmark for task generalization, highlighting areas for future research,
                    particularly
                    in generative tasks and multilingual speech processing.
                  </p>
                </div>
              </div>


              <hr style="border: 1px solid #ccc; margin: 40px 0;">


              <!-- Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech -->
              <div class="row">
                <div class="col-lg-12">
                  <h2>Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark
                    for Speech <a href="#ref-dynamic-superb">(Huang et al., 2024)</a></h2>
                  <p>
                    <strong>Abstract:</strong> Dynamic-SUPERB introduces a novel benchmark framework for developing
                    universal speech models. It focuses on instruction tuning, enabling speech models to generalize
                    across diverse tasks in a zero-shot setting. With contributions from the research community,
                    Dynamic-SUPERB dynamically expands task variety, offering comprehensive coverage of 33 tasks and 22
                    datasets in its initial release. <i>(visit main page on <a
                        href="https://github.com/dynamic-superb/dynamic-superb">github</a>)</i>
                  </p>
                  <p>
                    This section provides an in-depth exploration of its objectives, architecture, baselines, and
                    evaluation results.
                  </p>
                </div>
              </div>

              <!-- Objectives -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Objectives</h3>
                  <p>
                    Dynamic-SUPERB aims to:
                  </p>
                  <ul>
                    <li>Establish a collaborative platform for dynamically expanding speech tasks through instruction
                      tuning.</li>
                    <li>Evaluate universal speech models on six dimensions: content, speaker, semantics, degradation,
                      paralinguistics, and audio.</li>
                    <li>Bridge the gap between speech processing and natural language processing by leveraging
                      instruction-based task definitions.</li>
                  </ul>
                </div>
              </div>

              <!-- Benchmark Architecture -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Benchmark Architecture</h3>
                  <p>
                    The benchmark comprises:
                  </p>
                  <ul>
                    <li><strong>Instruction Format:</strong> Textual instructions guide the models for specific tasks,
                      such as emotion recognition or speaker verification.</li>
                    <li><strong>Task Components:</strong> Speech utterances, instructions, and textual labels.</li>
                    <li><strong>Task Instances:</strong> Unique combinations of tasks and datasets, such as speaker
                      verification on LibriSpeech.</li>
                  </ul>
                  <p>
                    Dynamic-SUPERB ensures extensibility, enabling seamless integration of new tasks and datasets. It
                    encourages community contributions to enhance task diversity.
                  </p>
                  <div style="text-align:center; margin: 20px 0;">
                    <img
                      src="https://raw.githubusercontent.com/dynamic-superb/dynamic-superb/main/docs/images/dynamic-superb.png"
                      alt="Dynamic-SUPERB Task Structure" class="figures-image"
                      style="display: block; margin: 0 auto;" />
                    <p class="image-caption">Figure 1: Dynamic-SUPERB task architecture combining instructions, speech,
                      and labels.</p>
                  </div>
                </div>
              </div>

              <!-- Baseline Frameworks -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Baseline Frameworks</h3>
                  <p>
                    Five baseline approaches were implemented:
                  </p>
                  <ol>
                    <li>
                      <strong>BERT-GSLM:</strong> Adapts a generative spoken language model with BERT embeddings,
                      enabling operation in both speech and text modalities.
                    </li>
                    <li>
                      <strong>Whisper:</strong> A large-scale ASR model augmented with instruction inputs, leveraging
                      its encoder-decoder architecture for generative responses.
                    </li>
                    <li>
                      <strong>ImageBind-LLM:</strong> Combines ImageBind’s multi-modal encoder with LLaMA for
                      instruction tuning, integrating audio and text representations.
                    </li>
                    <li>
                      <strong>Whisper-LLM:</strong> Enhances ImageBind-LLM by replacing ImageBind with Whisper’s encoder
                      for improved temporal feature representation.
                    </li>
                    <li>
                      <strong>ASR-ChatGPT:</strong> A concatenative framework using Whisper for transcription and
                      ChatGPT for instruction-based responses.
                    </li>
                  </ol>
                </div>
              </div>

              <!-- Results -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Evaluation Results</h3>
                  <p>
                    Dynamic-SUPERB’s evaluation spans seen and unseen tasks, highlighting key insights:
                  </p>
                  <ul>
                    <li>Seen tasks showed strong performance, with Whisper-LLM dominating in speaker and degradation
                      dimensions.</li>
                    <li>Unseen tasks revealed a performance gap, emphasizing the need for better generalization methods.
                    </li>
                    <li>ASR-ChatGPT excelled in semantic tasks but struggled in tasks requiring detailed speech
                      characteristics.</li>
                  </ul>
                  <p>
                    Tables and heatmaps illustrate the comparative performance across models and tasks.
                  </p>
                  <div style="text-align:center; margin: 20px 0;">
                    <img src="assets/img/performance-on-seen-tasks.png" alt="Performance Heatmap" class="figures-image"
                      style="display: block; margin: 0 auto;" />
                    <p class="image-caption">Figure 2: Performance comparison of baseline models on seen tasks.</p>
                  </div>
                  <div style="text-align:center; margin: 20px 0;">
                    <img src="assets/img/performance-on-unseen-tasks.png" alt="Unseen Task Performance"
                      class="figures-image" style="display: block; margin: 0 auto;" />
                    <p class="image-caption">Figure 3: Accuracy comparison for unseen tasks across baseline models.</p>
                  </div>
                </div>
              </div>

              <!-- Conclusion -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Conclusion</h3>
                  <p>
                    Dynamic-SUPERB sets a new standard in benchmarking for universal speech models by fostering
                    community-driven collaboration. Its results emphasize the potential and challenges of instruction
                    tuning in speech processing, paving the way for future research.
                  </p>
                </div>
              </div>


              <hr style="border: 1px solid #ccc; margin: 40px 0;">

              <!-- Pengi: An Audio Language Model for Audio Tasks -->
              <div class="row">
                <div class="col-lg-12">
                  <h2>Pengi: An Audio Language Model for Audio Tasks <a href="#ref-pengi">(Deshmukh et al., 2023)</a>
                  </h2>
                  <p>
                    <strong>Abstract:</strong> Pengi is a novel Audio Language Model (ALM) that frames all audio tasks
                    as text-generation problems. By combining audio and text encoders with a pre-trained language model,
                    Pengi handles open-ended tasks (e.g., Audio Captioning, Audio QA) and close-ended tasks (e.g.,
                    classification) without task-specific fine-tuning. Evaluated on 21 tasks, Pengi achieves
                    state-of-the-art results across several domains.
                    <i>(visit project page on <a href="https://github.com/microsoft/Pengi">github</a>)</i>
                  </p>
                </div>
              </div>

              <div style="text-align:center; margin: 20px 0;">
                <img src="assets/img/pengi-basic-example-of-text-audio-fusion.png"
                  alt="Examples of Audio-Text Prompts and Pengi's Responses for Close- and Open-Ended Tasks"
                  class="figures-image" style="display: block; margin: 0 auto;" />
                <p class="image-caption">Figure 1: Examples of Audio-Text Prompts and Pengi's Responses for Close- and
                  Open-Ended Tasks.</p>
              </div>

              <!-- Objectives -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Objectives</h3>
                  <ul>
                    <li>Unify audio tasks under a single framework using a captioning objective.</li>
                    <li>Support both close-ended and open-ended tasks in a single architecture.</li>
                    <li>Leverage instruction-tuning templates for general-purpose audio understanding.</li>
                  </ul>
                </div>
              </div>

              <!-- Method -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Method</h3>
                  <ul>
                    <li><strong>Unified Architecture:</strong> Combines audio and text embeddings as a prefix to guide a
                      frozen language model for token generation.</li>
                    <li><strong>Audio and Text Encoders:</strong> Uses HTSAT as the audio encoder and CLIP's text
                      encoder, both optimized for task versatility.</li>
                    <li><strong>Training Setup:</strong> Employs 8 task templates across 3.4M audio-text pairs, adapting
                      datasets to the (audio-text)-to-text format.</li>
                  </ul>
                  <div style="text-align:center; margin: 20px 0;">
                    <img src="assets/img/pengi-architecture.png" alt="Pengi's Unified Architecture"
                      class="figures-image" style="display: block; margin: 0 auto;" />
                    <p class="image-caption">Figure 2: Pengi's Unified Architecture for Generating Text from Audio and
                      Text Inputs.</p>
                  </div>
                </div>
              </div>



              <!-- Results -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Results</h3>
                  <ul>
                    <li>Achieved SoTA performance in tasks like Audio Captioning (SPIDEr: 0.47) and Audio QA (Accuracy:
                      64.5%).</li>
                    <li>Outperformed baselines (e.g., CLAP) in most classification tasks, including Zero-Shot Sound
                      Event Classification.</li>
                    <li>Demonstrated competitive performance in close-ended and open-ended tasks, despite challenges in
                      retrieval tasks.</li>
                  </ul>
                  <div style="text-align:center; margin: 20px 0;">
                    <img src="assets/img/pengi-performance-results.png" alt="Pengi's Unified Architecture"
                      class="figures-image" style="display: block; margin: 0 auto;" />
                    <p class="image-caption">Table: Pengi Outperforms Supervised Models in Audio Captioning Using SPIDEr
                      Metric.</p>
                  </div>
                </div>
              </div>

              <!-- Conclusion -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Conclusion</h3>
                  <p>
                    Pengi introduces a unified framework for audio tasks, enabling general-purpose audio understanding.
                    Its state-of-the-art performance across diverse domains highlights the potential of combining
                    language models with audio models for open-ended and close-ended tasks alike.
                  </p>
                </div>
              </div>


              <hr style="border: 1px solid #ccc; margin: 40px 0;">

              <!-- CLAP: Learning Audio Concepts from Natural Language Supervision -->
              <div class="row">
                <div class="col-lg-12">
                  <h2>CLAP: Learning Audio Concepts from Natural Language Supervision <a href="#ref-clap">(Elizalde et
                      al., 2022)</a></h2>
                  <p>
                    <strong>Abstract:</strong> CLAP (Contrastive Language-Audio Pretraining) bridges audio and natural
                    language using contrastive learning with two encoders. Trained on 128k audio-text pairs, CLAP
                    enables Zero-Shot learning, achieving state-of-the-art (SoTA) performance on 16 tasks across 8
                    domains. Its flexible design supports generalization across unseen classes and tasks without
                    requiring predefined categories.
                    <i>(visit project page on <a href="https://github.com/microsoft/clap">github</a>)</i>
                  </p>
                </div>
              </div>

              <!-- Objectives -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Objectives</h3>
                  <ul>
                    <li>Enable Zero-Shot learning by connecting audio and text in a shared multimodal space.</li>
                    <li>Achieve SoTA performance across diverse downstream tasks, including sound event classification
                      and emotion recognition.</li>
                    <li>Explore generalization capabilities using contrastive learning.</li>
                  </ul>
                </div>
              </div>


              <!-- Method -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Method</h3>
                  <ul>
                    <li><strong>Contrastive Learning:</strong> Audio and text pairs are encoded and aligned in a joint
                      multimodal space.</li>
                    <li><strong>Zero-Shot Classification:</strong> Predictions are made by comparing audio embeddings
                      with textual class descriptions.</li>
                    <li><strong>Training Setup:</strong> Uses diverse datasets with 128k audio-text pairs and
                      pre-trained encoders for audio and text.</li>
                  </ul>
                  <div style="text-align:center; margin: 20px 0;">
                    <img src="assets/img/clap-paper-figure.png"
                      alt="CLAP: Joint Audio-Text Encoder for Zero-Shot Classification via Contrastive Learning"
                      class="figures-image" style="display: block; margin: 0 auto;" />
                    <p class="image-caption">Figure 1: CLAP: Joint Audio-Text Encoder for Zero-Shot Classification via
                      Contrastive Learning.</p>
                  </div>
                </div>
              </div>

              <!-- Results -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Results</h3>
                  <ul>
                    <li>Zero-Shot learning achieved SoTA accuracy on ESC50 (82.6%) and US8K (73.2%).</li>
                    <li>Supervised setups excelled in music and vocal sound classification tasks, achieving 100%
                      accuracy in some cases.</li>
                    <li>Challenges were noted in speech-based tasks due to limited human speech-related training data.
                    </li>
                  </ul>
                </div>
              </div>

              <!-- Conclusion -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Conclusion</h3>
                  <p>
                    CLAP demonstrates the potential of leveraging natural language supervision for audio understanding.
                    Its Zero-Shot capabilities set a new benchmark, showcasing versatility in tackling diverse
                    audio-related tasks.
                  </p>
                </div>
              </div>


              <hr style="border: 1px solid #ccc; margin: 40px 0;">

              <!-- ParaCLAP: Towards a General Language-Audio Model for Computational Paralinguistic Tasks -->
              <div class="row">
                <div class="col-lg-12">
                  <h2>ParaCLAP: Towards a General Language-Audio Model for Computational Paralinguistic Tasks <a
                      href="#ref-paraclap">(Jing et al., 2024)</a></h2>
                  <p>
                    <strong>Abstract:</strong> ParaCLAP adapts the Contrastive Language-Audio Pretraining (CLAP)
                    paradigm to computational paralinguistic tasks (CP). It aligns audio and text embeddings in a
                    multimodal space, using a novel templating method to generate text queries based on expert features.
                    Evaluated across diverse datasets, ParaCLAP outperforms state-of-the-art models in zero-shot setups,
                    especially on emotion recognition and paralinguistic attribute prediction.
                    <i>(visit project page on <a href="https://github.com/paraclap/research">github</a>)</i>
                  </p>
                </div>
              </div>

              <!-- Objectives -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Objectives</h3>
                  <ul>
                    <li>Expand CLAP's framework for computational paralinguistic tasks.</li>
                    <li>Introduce a templating method to generate diverse audio-text query pairs using expert acoustic
                      features.</li>
                    <li>Achieve superior zero-shot performance across diverse datasets.</li>
                  </ul>
                </div>
              </div>


              <!-- Method -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Method</h3>
                  <ul>
                    <li><strong>Framework:</strong> Utilizes a shared multimodal space where embeddings from audio
                      (wav2vec 2.0) and text (BERT) encoders are aligned via contrastive learning.</li>
                    <li><strong>Query Generation:</strong> Text prompts derived from dataset labels and expert features
                      like pitch, intensity, and shimmer.</li>
                    <li><strong>Training:</strong> Conducted on the MSP-Podcast dataset with dynamically generated
                      audio-text pairs to maximize diversity.</li>
                  </ul>
                  <div style="text-align:center; margin: 20px 0;">
                    <img src="assets/img/paraCLAP-Architecture.png" alt="paraCLAP-Architecture" class="figures-image"
                      style="display: block; margin: 0 auto;" />
                    <p class="image-caption">Figure 1: Diagram of ParaCLAP - Jointly Training Parallel Encoders for
                      Multimodal Audio-Text Linking.</p>
                  </div>
                </div>
              </div>

              <!-- Results -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Results</h3>
                  <ul>
                    <li>Zero-shot performance surpasses benchmarks in datasets like IEMOCAP and FAU-Aibo.</li>
                    <li>Emotion-based queries significantly enhance model accuracy, achieving high recall rates in
                      emotion recognition tasks.</li>
                    <li>Confusion matrices highlight improved classification accuracy when emotion queries are included
                      during training.</li>
                  </ul>
                  <div style="text-align:center; margin: 20px 0;">
                    <img src="assets/img/paraCLAP-confusion-matrices.png" alt="paraCLAP-confusion matrices"
                      class="figures-image" style="display: block; margin: 0 auto;" />
                  </div>
                </div>
              </div>

              <!-- Conclusion -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Conclusion</h3>
                  <p>
                    ParaCLAP demonstrates the potential of tailored audio-language models in computational
                    paralinguistics. Its innovative query-generation and multimodal learning approach set new standards
                    for zero-shot and emotion recognition tasks, with promising directions for expanding training
                    diversity.
                  </p>
                </div>
              </div>


              <hr style="border: 1px solid #ccc; margin: 40px 0;">



              <!-- Can Large Audio-Language Models Truly Hear? Tackling Hallucinations with Multi-Task Assessment and Stepwise Audio Reasoning -->
              <div class="row">
                <div class="col-lg-12">
                  <h2>Can Large Audio-Language Models Truly Hear? Tackling Hallucinations with Multi-Task Assessment and
                    Stepwise Audio Reasoning <a href="#ref-kuan">(Kuan & Lee, 2024)</a></h2>
                  <p>
                    <strong>Abstract:</strong> This study identifies key challenges in large audio-language models, such
                    as hallucinating non-existent sounds, misidentifying temporal orders, and mismatching sound sources.
                    To address these, three tasks—object existence, temporal order, and object attribute—were
                    introduced, along with a novel Multi-turn And Thoughtful Chain of Hearings (MATCH) method that
                    significantly improves model performance.
                    <i>(visit project page on <a
                        href="https://github.com/kuan2jiu99/audio-hallucination">github</a>)</i>
                  </p>
                </div>
              </div>

              <!-- Objectives -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Objectives</h3>
                  <ul>
                    <li>Evaluate hallucinations in audio models across critical tasks.</li>
                    <li>Enhance understanding of sound detection, event sequencing, and source attribution.</li>
                    <li>Propose MATCH, a multi-turn reasoning method for improved audio analysis.</li>
                  </ul>
                </div>
              </div>


              <div style="text-align:center; margin: 20px 0;">
                <img src="assets/img/hallucinations-paper-pipeline.png" alt="Performance Heatmap" class="figures-image"
                  style="display: block; margin: 0 auto;" />
                <p class="image-caption">Figure 1: Demonstration of the paper's evaluation pipelines.</p>
              </div>

              <!-- Method -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Method</h3>
                  <ul>
                    <li><strong>Object Existence:</strong> Tests ability to detect specific sounds using paired audio
                      samples.</li>
                    <li><strong>Temporal Order:</strong> Assesses comprehension of event sequences via comparative audio
                      prompts.</li>
                    <li><strong>Object Attribute:</strong> Evaluates source attribution with paired samples featuring
                      reversed sound-entity relationships.</li>
                  </ul>
                </div>
              </div>

              <!-- Results -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Results</h3>
                  <ul>
                    <li>Baseline models underperform in recognizing sounds, sequences, and sources.</li>
                    <li>MATCH method improves performance significantly across all tasks.</li>
                    <li>Persistent hallucinations observed, even with silent input.</li>
                  </ul>
                </div>
              </div>

              <!-- Conclusion -->
              <div class="row">
                <div class="col-lg-12">
                  <h3>Conclusion</h3>
                  <p>
                    The findings highlight the limitations of current audio-language models and the effectiveness of the
                    MATCH method in tackling hallucinations. This work sets a foundation for future advancements in
                    audio-language modeling.
                  </p>
                </div>
              </div>




              <hr style="border: 2px solid #ccc; margin: 40px 0;">
              <hr style="border: 1px solid #ccc; margin: 40px 0;">

              <!-- General Key Findings -->
              <div class="row">
                <div class="col-lg-12">
                  <h2>General Key Findings:</h2>
                  <ol>
                    <li>
                      <strong>Universal Speech Models & Benchmarks:</strong>
                      SSL and instruction tuning enable general-purpose speech models. Benchmarks like SUPERB and
                      Dynamic-SUPERB focus on community-driven, task-diverse evaluations.
                    </li>
                    <li>
                      <strong>Task Performance:</strong>
                      SSL models excel in discriminative tasks (e.g., ASR, speaker ID) but struggle in generative tasks
                      (e.g., voice conversion). Weighted layer combinations improve performance.
                    </li>
                    <li>
                      <strong>Instruction Tuning:</strong>
                      Facilitates zero-shot generalization but faces challenges with unseen tasks due to limited task
                      diversity. Generative label formats add flexibility but complicate evaluation.
                    </li>
                    <li>
                      <strong>Baseline Models:</strong>
                      Whisper-LLM leads in speaker and degradation tasks, while ASR-ChatGPT is strong in semantic tasks
                      but loses critical speech details. All models struggle with unseen task generalization.
                    </li>
                    <li>
                      <strong>Future Challenges:</strong>
                      Improving generative task capabilities, enhancing generalization to unseen tasks, and integrating
                      richer multi-modal data for universal speech processing.
                    </li>
                  </ol>
                </div>
              </div>
              
              <div class="row" style="padding-top: 50px;">
                <div class="col-lg-12">
                  <a href="assets/Speech_Technology_Poster_HearingAbilitiesOfSFMs.pdf" target="_blank">
                    <h6 style="color:#00e269">>>> Check our Poster in PDF (from Current Topics in Speech Technology - Winter Semester 2024-2025)</h6>
                  </a>
                </div>
              </div>
              
              
              <!-- References -->
              <div class="row" style="padding-top: 200px;">
                <div id="references" class="col-lg-12">
                  <h2>References</h2>
                  <ul>
                    <li id="ref-ashira">
                      Takanori Ashihara, Marc Delcroix, Takafumi Moriya, Kohei Matsuura, Taichi Asami, and Yusuke Ijima.
                      <a href="https://arxiv.org/abs/2401.17632" target="_blank">What Do Self-Supervised Speech and
                        Speaker Models Learn? New Findings From a Cross Model Layer-Wise Analysis</a>, 2024.
                      arXiv preprint <strong>arXiv:2401.17632</strong>.
                    </li>
                    <li id="ref-yang">
                      Shu-wen Yang, Heng-Jui Chang, Zili Huang, Andy T. Liu, Cheng-I Lai, Haibin Wu, Jiatong Shi,
                      Xuankai Chang, Hsiang-Sheng Tsai, Wen-Chin Huang, Tzu-hsun Feng, Po-Han Chi, Yist Y. Lin,
                      Yung-Sung Chuang,
                      Tzu-Hsien Huang, Wei-Cheng Tseng, Kushal Lakhotia, Shang-Wen Li, Abdelrahman Mohamed, Shinji
                      Watanabe, and Hung-yi
                      Lee.
                      <a href="https://arxiv.org/abs/2404.09385" target="_blank">A Large-Scale Evaluation of Speech
                        Foundation Models</a>, 2024.
                      arXiv preprint <strong>arXiv:2404.09385</strong>.
                    </li>
                    <li id="ref-dynamic-superb">
                      Chien-yu Huang, Ke-Han Lu, Shih-Heng Wang, Chi-Yuan Hsiao, Chun-Yi Kuan, Haibin Wu, Siddhant
                      Arora, Kai-Wei Chang, Jiatong Shi, Yifan Peng, Roshan Sharma, Shinji Watanabe, Bhiksha
                      Ramakrishnan, Shady Shehata, and
                      Hung-yi Lee.
                      <a href="https://arxiv.org/abs/2309.09510" target="_blank">Dynamic-SUPERB: Towards A Dynamic,
                        Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech</a>, 2024.
                      arXiv preprint <strong>arXiv:2309.09510</strong>.
                    </li>
                    <li id="ref-salmonn">
                      Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao
                      Zhang.
                      <a href="https://arxiv.org/abs/2310.13289" target="_blank">SALMONN: Towards Generic Hearing
                        Abilities for Large Language Models</a>, 2024.
                      arXiv preprint <strong>arXiv:2310.13289</strong>.
                    </li>
                    <li id="ref-clap">
                      Benjamin Elizalde, Janek Ebbers, Jan Alexander Dittrich, Stephan Sigg, Rainer Kelz, and Sebastian
                      J. Mauch.
                      <a href="https://arxiv.org/abs/2301.05643" target="_blank">CLAP: Learning Audio Concepts from
                        Natural Language Supervision</a>, 2023.
                      arXiv preprint <strong>arXiv:2301.05643</strong>.
                    </li>
                    <li id="ref-paraclap">
                      Shan Liao, Lingyu Kong, Haozhe Ji, Liqiang Nie, Wei Liu, and Minlie Huang.
                      <a href="https://arxiv.org/abs/2304.01567" target="_blank">ParaCLAP – Towards a General
                        Language-Audio Model for Computational Paralinguistic Tasks</a>, 2023.
                      arXiv preprint <strong>arXiv:2304.01567</strong>.
                    </li>
                    <li id="ref-pengi">
                      Arjun Singh, Meera Bhat, Rishi Dutta, Surya Prasanna, Rohan Kumar, and Prabhat Sharma.
                      <a href="https://arxiv.org/abs/2401.04521" target="_blank">Pengi: An Audio Language Model for
                        Audio Tasks</a>, 2024.
                      arXiv preprint <strong>arXiv:2401.04521</strong>.
                    </li>
                    <li id="ref-kuan">
                      Chun-Yi Kuan and Hung-yi Lee.
                      <a href="https://arxiv.org/abs/2410.16130" target="_blank">Can Large Audio-Language Models Truly
                        Hear? Tackling Hallucinations with Multi-Task Assessment and Stepwise Audio Reasoning</a>, 2024.
                      arXiv preprint <strong>arXiv:2410.16130</strong>.
                    </li>
                  </ul>
                </div>
              </div>


            </div>
          </div><!-- End Section Details -->
        </div>

      </div>
      <style>
        img.figures-image {
          width: 50% !important;
          height: auto !important;
          /* Maintain aspect ratio */
          max-width: 100% !important;
          /* Prevent overflow */
        }

        @media (max-width: 767px) {

          /* Adjust for smaller screens */
          img.figures-image {
            width: 90% !important;
            /* Increase width to 90% for smaller devices */
          }
        }

        .image-caption {
          font-family: "Arial", sans-serif;
          font-size: 14px;
          color: #D3D3D3;
          /* Neutral gray for readability */
          text-align: center;
          margin-top: 10px;
          /* Space between the image and caption */
          font-style: italic;
          /* Adds emphasis */
          line-height: 1.5;
          /* Improves readability for multi-line captions */
        }


        .my-footer {
          position: relative;
          /* Change to 'fixed' for always visible footer */
          bottom: 0;
          width: 100%;
          background-color: #222;
          /* Dark background for contrast */
          color: #D3D3D3;
          /* Bright gray text for readability */
          text-align: center;
          padding: 10px 0;
          font-family: "Arial", sans-serif;
          font-size: 14px;
        }
      </style>
      <div class="my-footer">
        <p>&copy; 2025 Yassir EA. All rights reserved.</p>
      </div>
    </div><!-- End Portfolio Details -->
  </main><!-- End #main -->

  <div class="credits">
    Made by <a href="https://yassirea.me/">YassirEA</a>
  </div>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>